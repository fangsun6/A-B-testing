# ğŸ® A/B Testing: Impact of Gate Level on Player Retention in Cookie Cats

This project investigates how changing the placement of the first in-game gate in the mobile puzzle game **Cookie Cats** affects player behaviorâ€”particularly retention and gameplay engagementâ€”using robust statistical testing and visualization techniques.

---

## ğŸ“Œ Problem Statement

Cookie Cats features â€œgatesâ€ that restrict player progression unless they wait or pay. Originally placed at **level 30**, Tactile Entertainment tested whether delaying the first gate to **level 40** would improve player **Day 1** and **Day 7 retention**, and **gameplay engagement**.

We aim to answer:
- âœ… Does moving the gate improve short- or long-term retention?
- ğŸ¯ Does it alter how much players engage with the game?

---

## ğŸ“Š Dataset Overview

- **File**: `cookie_cats.csv`
- **Size**: ~90,000 rows Ã— 5 columns
- **Columns**:
  - `userid`: Unique player ID
  - `version`: Group label (`gate_30` or `gate_40`)
  - `sum_gamerounds`: Number of rounds played in the first week
  - `retention_1`: Returned next day
  - `retention_7`: Returned a week later

---

## ğŸ“‰ Key Metrics

- **Engagement**: `sum_gamerounds` â€” Total gameplay rounds
- **Retention**: Binary indicators `retention_1` and `retention_7`
- **Composite Metric**: Combined Day 1 and Day 7 retention

---

## ğŸ§¼ Data Cleaning & Preprocessing

### ğŸš§ Main Challenges:
| Challenge | Impact |
|----------|--------|
| **Extreme Outliers** | Some players logged 49k+ roundsâ€”artificially inflated means and visual distortion |
| **Zero-round Players** | 4.4% of players didnâ€™t play at allâ€”skewed retention but irrelevant for engagement |
| **Heavy Skewness** | Most played <50 rounds, a few played 1000+ â€” mean is misleading |
| **Overplotting in Visuals** | Large volume made boxplots and histograms hard to interpret |

### ğŸ§½ Cleaning Strategy:
| Step | Action | Rationale |
|------|--------|-----------|
| 1 | Removed top outlier (`max(sum_gamerounds)`) | Prevented single-player distortion |
| 2 | Used 1stâ€“99th percentile range | Defined realistic player behavior |
| 3 | Treated zero-round users differently | Included for retention, excluded for engagement |
| 4 | Compared plots before/after cleaning | Verified improvement in clarity |

---

## ğŸ“Š Visualization & Interpretation

### ğŸ” Visualization Goals:
- Understand group-level behavior
- Reveal skew and outliers
- Validate test assumptions
- Support hypothesis testing

### ğŸ”§ Tools Used:
- **Histograms & Boxplots** â€” Engagement spread
- **Density/ECDF plots** â€” Retention distribution
- **Stacked Bar Charts** â€” Retention pattern breakdown

### ğŸ¯ Observations:
- Most players quit early (low engagement)
- Minimal visual difference between groups post-cleaning
- Clear long-tail in engagement that needed trimming
- Day 7 retention differences only visible after aggregation

---

## ğŸ§ª A/B Testing Design

| Group | Description |
|-------|-------------|
| **A** | Gate at level 30 (control) |
| **B** | Gate at level 40 (test) |

- **Randomized**: Players were randomly assigned
- **Binary & Numeric Metrics**: Analyzed both engagement and retention
- **Statistical Validity**: Checked assumptions before testing

---

## ğŸ§  Statistical Testing Strategy

### 1. Pre-Test Diagnostics
- **Shapiro-Wilk Test**: Normality
- **Levene's Test**: Variance equality

### 2. Statistical Test Selection:
| Conditions | Test Used |
|------------|-----------|
| Normal + Equal Variance | Student's t-test |
| Normal + Unequal Variance | Welchâ€™s t-test |
| Non-normal | Mannâ€“Whitney U test |

### 3. Bootstrapping (for robustness)
- 10,000 resamples to build sampling distribution
- Calculated confidence intervals and effect distributions

---

## ğŸ§ª Binary Metric Testing (Retention)

Retention variables are binary (`True`/`False`) â€” we used:
- **Proportion comparison (Z-test)**:
  \
  z = (p1 - p2) / sqrt(p(1-p)(1/n1 + 1/n2))
  \
- **Bootstrapping**: To get confidence intervals on proportions

---

## ğŸ” Bootstrapping Analysis: Why and How

### ğŸš§ Key Issues:
| Problem | Solution |
|---------|----------|
| **Non-normality** | Used resampling to build empirical distributions |
| **Outliers** | Trimmed extreme values, focused on medians |
| **Computational Cost** | Limited resamples, used `tqdm` for progress |
| **Small effect sizes** | Visualized effect size distribution to aid interpretation |

---

## ğŸ§ª Experiment Results

### ğŸ“Š Engagement (Game Rounds)

- **Test Used**: Mannâ€“Whitney U (non-parametric)
- **p-value**: 0.0509  
- âŒ No statistically significant difference at Î± = 0.05

### ğŸ“Š Retention (Day 1 and Day 7)

| Metric | Bootstrap 95% CI | Result |
|--------|------------------|--------|
| **Day 1** | [-0.002, 0.012] | âŒ Not significant |
| **Day 7** | [0.008, 0.022] | âœ… Statistically significant |

---

## ğŸ¯ Final Conclusions

| Metric | Impact of Moving Gate to Level 40 |
|--------|----------------------------------|
| **Day 1 Retention** | No negative effect |
| **Day 7 Retention** | Moderate improvement (statistically significant) |
| **Engagement** | No significant change |

> ğŸ’¡ **Key Takeaway**: Moving the gate delayed friction without harming short-term retentionâ€”and **possibly enhanced long-term loyalty**.

---

## ğŸ¤” Reflection: Thinking Flow & Lessons Learned

### ğŸ§  Thinking Process
- Understand game design change â†’ identify measurable KPIs
- Clean the data with statistical integrity
- Use EDA to form early hypotheses and guide test choices
- Choose proper tests based on assumptions, not habit
- Use bootstrapping to validate sensitive findings

### âš ï¸ Project Challenges

| Challenge | Resolution |
|----------|------------|
| Skewed data | Used percentiles and log/box-cox inspection |
| Binary metrics | Z-tests and bootstrapped intervals |
| Interpretation | Combined visualization + statistical outputs |
| Visual clutter | Used filtered histograms, ECDFs, group splits |
| Marginal p-values | Validated with bootstrap CI, not just p-values |

---

## ğŸ“Œ Summary

This A/B test shows how even small design changes (like level gating) can affect long-term user retention. A careful approach involving robust cleaning, tailored statistical testing, and bootstrapping helped ensure trustworthy conclusions.
